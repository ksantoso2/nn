{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "\n",
        "torch.manual_seed(47)\n",
        "np.random.seed(47)\n",
        "random.seed(47)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfZqPtiHCfMn",
        "outputId": "4472468c-6d3a-4235-88f9-135b34befdb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fashion_mnist(batch_size=64, subset_fraction=0.2, selected_classes=None):\n",
        "    transform_train = transforms.Compose([\n",
        "      transforms.Resize((32, 32)),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.RandomRotation(10),\n",
        "      transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # convert 1 channel to 3\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "      transforms.Resize((32, 32)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
        "    ])\n",
        "\n",
        "    train_dataset = FashionMNIST(root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "    test_dataset = FashionMNIST(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "\n",
        "    # filter by classes and subset data\n",
        "    if selected_classes is not None:\n",
        "        train_indices = [i for i, (_, label) in enumerate(train_dataset) if label in selected_classes]\n",
        "        test_indices = [i for i, (_, label) in enumerate(test_dataset) if label in selected_classes]\n",
        "        train_dataset = Subset(train_dataset, train_indices)\n",
        "        test_dataset = Subset(test_dataset, test_indices)\n",
        "\n",
        "    if subset_fraction < 1.0:\n",
        "        train_size = int(len(train_dataset) * subset_fraction)\n",
        "        test_size = int(len(test_dataset) * subset_fraction)\n",
        "\n",
        "        train_indices = random.sample(range(len(train_dataset)), train_size)\n",
        "        test_indices = random.sample(range(len(test_dataset)), test_size)\n",
        "\n",
        "        train_subset = Subset(train_dataset, train_indices)\n",
        "        test_subset = Subset(test_dataset, test_indices)\n",
        "    else:\n",
        "        train_subset = train_dataset\n",
        "        test_subset = test_dataset\n",
        "\n",
        "    # split training data and create validation sets\n",
        "    train_size = int(0.9 * len(train_subset))\n",
        "    val_size = len(train_subset) - train_size\n",
        "    train_dataset, valid_dataset = random_split(train_subset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_subset, batch_size=batch_size)\n",
        "\n",
        "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "    if selected_classes:\n",
        "        # filter class names\n",
        "        class_names = [class_names[i] for i in selected_classes]\n",
        "\n",
        "    print(f\"Training set size: {len(train_dataset)}\")\n",
        "    print(f\"Validation set size: {len(valid_dataset)}\")\n",
        "    print(f\"Test set size: {len(test_subset)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_names"
      ],
      "metadata": {
        "id": "xm4W0tyWHQ_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzLSnKi_E75g"
      },
      "outputs": [],
      "source": [
        "# patches\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=4, embed_dim=64, img_size=32):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # [batch_size, embed_dim, grid_size, grid_size]\n",
        "        x = x.flatten(2)  # [batch_size, embed_dim, num_patches]\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_patches, embed_dim]\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        in_channels=3,\n",
        "        embed_dim=64,\n",
        "        depth=6,\n",
        "        num_heads=4,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1,\n",
        "        attn_dropout=0.1,\n",
        "        embedding_dropout=0.1,\n",
        "        embedding_dim=128,\n",
        "        num_classes=10\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # oatch embedding\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            in_channels=in_channels,\n",
        "            patch_size=patch_size,\n",
        "            embed_dim=embed_dim,\n",
        "            img_size=img_size\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # class token and positional embedding\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "\n",
        "        # embedding dropout\n",
        "        self.dropout = nn.Dropout(embedding_dropout)\n",
        "\n",
        "        # transformer encoder\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=int(embed_dim * mlp_ratio),\n",
        "                dropout=dropout,\n",
        "                activation=\"gelu\",\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=depth\n",
        "        )\n",
        "\n",
        "        # mlp head for embeddings\n",
        "        self.embedding_layer = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, embedding_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # classification head\n",
        "        self.head = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "        # initialize weights\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x, return_embeddings=False):\n",
        "        # patch embedding\n",
        "        x = self.patch_embed(x)  # [batch_size, num_patches, embed_dim]\n",
        "\n",
        "        # add class token\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)  # [batch_size, num_patches + 1, embed_dim]\n",
        "\n",
        "        # add positional embedding\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # transformer\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # class token for classification\n",
        "        cls_token_final = x[:, 0]\n",
        "\n",
        "        # embedding layer\n",
        "        embeddings = self.embedding_layer(cls_token_final)\n",
        "\n",
        "        # classification head\n",
        "        logits = self.head(embeddings)\n",
        "\n",
        "        if return_embeddings:\n",
        "            return logits, embeddings\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "# triplet loss (CHANGE THIS!!)\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        positive_dist = torch.sum((anchor - positive) ** 2, dim=1)\n",
        "        negative_dist = torch.sum((anchor - negative) ** 2, dim=1)\n",
        "\n",
        "        losses = F.relu(positive_dist - negative_dist + self.margin)\n",
        "        return torch.mean(losses)\n",
        "\n",
        "\n",
        "def train_with_metric_learning(model, train_loader, val_loader, optimizer, scheduler, num_epochs=15, save_path='best_model.pth'):\n",
        "    classification_criterion = nn.CrossEntropyLoss()\n",
        "    triplet_criterion = TripletLoss(margin=1.0)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_cls_loss = 0.0\n",
        "        running_triplet_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "        for inputs, labels in train_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, embeddings = model(inputs, return_embeddings=True)\n",
        "            cls_loss = classification_criterion(outputs, labels)\n",
        "            triplet_loss = torch.tensor(0.0).to(device)\n",
        "            unique_labels = torch.unique(labels)\n",
        "\n",
        "            # only compute triplet loss if we have at least 2 classes in the batch\n",
        "            if len(unique_labels) >= 2:\n",
        "                for label in unique_labels:\n",
        "                    mask_anchor = (labels == label)\n",
        "                    mask_negative = (labels != label)\n",
        "\n",
        "                    if mask_anchor.sum() >= 2 and mask_negative.sum() >= 1:\n",
        "                        anchor_indices = torch.where(mask_anchor)[0]\n",
        "                        positive_indices = anchor_indices[torch.randperm(len(anchor_indices))]\n",
        "\n",
        "                        mask_diff = (anchor_indices != positive_indices)\n",
        "                        if mask_diff.sum() > 0:\n",
        "                            anchor_indices = anchor_indices[mask_diff][:1]  # Take just one\n",
        "                            positive_indices = positive_indices[mask_diff][:1]  # Take just one\n",
        "\n",
        "                            negative_indices = torch.where(mask_negative)[0]\n",
        "                            negative_indices = negative_indices[torch.randperm(len(negative_indices))][:1]  # Take just one\n",
        "\n",
        "                            anchor_embeds = embeddings[anchor_indices]\n",
        "                            positive_embeds = embeddings[positive_indices]\n",
        "                            negative_embeds = embeddings[negative_indices]\n",
        "\n",
        "                            batch_triplet_loss = triplet_criterion(anchor_embeds, positive_embeds, negative_embeds)\n",
        "                            triplet_loss += batch_triplet_loss\n",
        "\n",
        "            loss = cls_loss + 0.5 * triplet_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # training statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_cls_loss += cls_loss.item() * inputs.size(0)\n",
        "            running_triplet_loss += triplet_loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            train_bar.set_postfix(\n",
        "                loss=loss.item(),\n",
        "                cls_loss=cls_loss.item(),\n",
        "                triplet=triplet_loss.item(),\n",
        "                acc=correct/total\n",
        "            )\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_cls_loss = running_cls_loss / len(train_loader.dataset)\n",
        "        epoch_triplet_loss = running_triplet_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = correct / total\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "\n",
        "        print(f\"Train - Cls Loss: {epoch_cls_loss:.4f}, Triplet Loss: {epoch_triplet_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "            for inputs, labels in val_bar:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = classification_criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                val_bar.set_postfix(loss=loss.item(), acc=correct/total)\n",
        "\n",
        "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
        "        epoch_val_acc = correct / total\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"New best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "    return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return accuracy, np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "def extract_embeddings(model, data_loader):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(data_loader, desc=\"Extracting embeddings\"):\n",
        "            inputs = inputs.to(device)\n",
        "            _, batch_embeddings = model(inputs, return_embeddings=True)\n",
        "            embeddings.append(batch_embeddings.cpu().numpy())\n",
        "            labels.append(targets.numpy())\n",
        "\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    labels = np.concatenate(labels)\n",
        "\n",
        "    return embeddings, labels\n",
        "\n",
        "def compute_distance_matrix(embeddings, metric='euclidean'):\n",
        "    \"\"\"Compute pairwise distances between embeddings\"\"\"\n",
        "    if metric == 'euclidean':\n",
        "        sq_dists = torch.cdist(\n",
        "            torch.tensor(embeddings),\n",
        "            torch.tensor(embeddings),\n",
        "            p=2\n",
        "        ).square().numpy()\n",
        "        return sq_dists\n",
        "    elif metric == 'cosine':\n",
        "        # Cosine similarity\n",
        "        normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "        similarities = np.dot(normalized, normalized.T)\n",
        "        return 1 - similarities\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "def precision_at_k(distance_matrix, labels, k=5):\n",
        "    n = distance_matrix.shape[0]\n",
        "    neighbors = np.argsort(distance_matrix, axis=1)[:, 1:k+1]  # skip the first one\n",
        "\n",
        "    precision = 0\n",
        "    for i in range(n):\n",
        "        query_label = labels[i]\n",
        "        neighbor_labels = labels[neighbors[i]]\n",
        "        precision += np.mean(neighbor_labels == query_label)\n",
        "\n",
        "    return precision / n\n",
        "\n",
        "# FIX THIS!!!\n",
        "def visualize_neighbors(data_loader, embeddings, labels, class_names, num_queries=5, k=5):\n",
        "    \"\"\"Visualize query images and their nearest neighbors\"\"\"\n",
        "    all_images = []\n",
        "    all_indices = []\n",
        "\n",
        "    for batch_idx, (images, batch_labels) in enumerate(data_loader):\n",
        "        all_images.append(images)\n",
        "        all_indices.extend(range(batch_idx * data_loader.batch_size,\n",
        "                          min((batch_idx + 1) * data_loader.batch_size, len(data_loader.dataset))))\n",
        "        if len(all_indices) >= 1000:\n",
        "            break\n",
        "\n",
        "    all_images = torch.cat(all_images, dim=0)\n",
        "\n",
        "    distances = compute_distance_matrix(embeddings)\n",
        "\n",
        "    query_indices = np.random.choice(len(all_indices), num_queries, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, num_queries * 2))\n",
        "\n",
        "    for i, query_idx in enumerate(query_indices):\n",
        "        real_query_idx = all_indices[query_idx]\n",
        "        query_label = labels[real_query_idx]\n",
        "\n",
        "        neighbor_indices = np.argsort(distances[real_query_idx])\n",
        "        neighbor_indices = neighbor_indices[1:k+1]  # skip the first one\n",
        "\n",
        "        plt.subplot(num_queries, k+1, i*(k+1) + 1)\n",
        "        img = all_images[query_idx].permute(1, 2, 0).numpy()\n",
        "        img = img[:, :, 0]  # take only first channel\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"Query: {class_names[query_label]}\", fontsize=8)\n",
        "        plt.axis('off')\n",
        "\n",
        "        for j, neighbor_idx in enumerate(neighbor_indices):\n",
        "            real_neighbor_idx = all_indices[neighbor_idx]\n",
        "            neighbor_label = labels[real_neighbor_idx]\n",
        "\n",
        "            plt.subplot(num_queries, k+1, i*(k+1) + j + 2)\n",
        "            img = all_images[neighbor_idx].permute(1, 2, 0).numpy()\n",
        "            img = img[:, :, 0]\n",
        "            plt.imshow(img, cmap='gray')\n",
        "\n",
        "            color = 'green' if neighbor_label == query_label else 'red'\n",
        "            plt.title(f\"{class_names[neighbor_label]}\", fontsize=8, color=color)\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('fashion_mnist_neighbors.png')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_embeddings(embeddings, labels, class_names):\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)\n",
        "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    scatter = plt.scatter(\n",
        "        reduced_embeddings[:, 0],\n",
        "        reduced_embeddings[:, 1],\n",
        "        c=labels,\n",
        "        cmap='tab10',\n",
        "        alpha=0.7,\n",
        "        s=10\n",
        "    )\n",
        "    plt.colorbar(scatter, ticks=range(len(class_names)))\n",
        "    plt.title('t-SNE visualization of Fashion MNIST embeddings')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    handles, _ = scatter.legend_elements()\n",
        "    plt.legend(handles, class_names, loc=\"upper right\", title=\"Classes\")\n",
        "\n",
        "    plt.savefig('fashion_mnist_embeddings_pytorch.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_history(train_losses, val_losses, train_accs, val_accs):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('fashion_mnist_training_history_pytorch.png')\n",
        "    plt.show()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    num_epochs = 50\n",
        "    learning_rate = 3e-4\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    # subset data (CHANGE!)\n",
        "    subset_fraction = 0.7\n",
        "    selected_classes = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "    train_loader, val_loader, test_loader, class_names = load_fashion_mnist(\n",
        "        batch_size=batch_size,\n",
        "        subset_fraction=subset_fraction,\n",
        "        selected_classes=selected_classes\n",
        "    )\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    model = VisionTransformer(\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        in_channels=3,\n",
        "        embed_dim=64,\n",
        "        depth=6,\n",
        "        num_heads=4,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1,\n",
        "        embedding_dim=128,\n",
        "        num_classes=num_classes\n",
        "    ).to(device)\n",
        "\n",
        "    print(model)\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"number of parameters: {num_params:,}\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "    model_save_path = 'results/best_model.pth'\n",
        "\n",
        "    train_losses, val_losses, train_accs, val_accs = train_with_metric_learning(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        num_epochs=num_epochs,\n",
        "        save_path=model_save_path\n",
        "    )\n",
        "\n",
        "    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
        "    model.load_state_dict(torch.load(model_save_path))\n",
        "    accuracy, all_preds, all_labels = evaluate_model(model, test_loader)\n",
        "    test_embeddings, test_labels = extract_embeddings(model, test_loader)\n",
        "    visualize_embeddings(test_embeddings, test_labels, class_names)\n",
        "\n",
        "    distances = compute_distance_matrix(test_embeddings)\n",
        "    prec_at_5 = precision_at_k(distances, test_labels, k=5)\n",
        "    print(f\"precision for retrieval: {prec_at_5:.4f}\")\n",
        "\n",
        "    visualize_neighbors(test_loader, test_embeddings, test_labels, class_names)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}